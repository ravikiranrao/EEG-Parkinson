{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This file is used for classification of emotion-based EEG signals from PD and HC using 2D Convolutional Neural Network (2D-CNN). The input to 2D-CNN, *EEG Images*, are obtained using `eeg_to_images.ipynb` file. The detailed architecture, input and output dimensions are explained in the paper. The highlighted part in the following image indicate the pipeline of the current file.\n",
    "\n",
    "Note: The configurations, like hyperparameter values, present in this file are not the final configurations as reported in the paper. The present configurations are maybe mutated for additional experiments. However, results reported in the paper can be reproduced by plugging in the appropriate configurations as mentioned in the paper.\n",
    "\n",
    "![2D CNN pipeline associated with this notebook](images/2dcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score,roc_auc_score, roc_curve, auc\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, LeaveOneGroupOut\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7412, 3, 32, 32)\n",
      "Features shape: (7412, 32, 32, 3)\n",
      "Number of classes: 6\n",
      "Unique labels: [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Path to the pre-processed features\n",
    "dataPath = '../data/spectral_feat_tensor_full_with_full_labels_images.mat'\n",
    "dataPath2 = '../data/spectral_feat_tensor_full_with_full_labels_subject_id.mat'\n",
    "\n",
    "# Path to save features, results, etc.\n",
    "savePath = '../results/CNN_2D_our/'\n",
    "\n",
    "# Experiment name\n",
    "experiment = 'MultiClass_NC'\n",
    "\n",
    "filename = savePath+'CNN_2D_results_'+experiment+'.mat'\n",
    "plot_title = 'Categorical Emotion Classification - NC'\n",
    "\n",
    "# --- Model parameters ---\n",
    "\n",
    "nb_filters = [16, 32, 32, 64, 128]  # Number of filters for convolution\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "stride_size = 2\n",
    "padding = 'same'\n",
    "weight_decay = 0.000001\n",
    "dense_layer_neuron_num = 128\n",
    "epochs = 30\n",
    "momentum =0.8\n",
    "\n",
    "matContent = sio.loadmat(dataPath)\n",
    "matContent2 = sio.loadmat(dataPath2)\n",
    "features = matContent['nc_feat_img']\n",
    "print(features.shape)\n",
    "subject_ids = np.squeeze(matContent['subject_id'])\n",
    "labels = np.squeeze(matContent['nc_multi_labels'])\n",
    "labels[labels < 0] = 0\n",
    "features = np.swapaxes(features,1,3)\n",
    "features = np.swapaxes(features,1,2)\n",
    "labels[labels == 6] = 0\n",
    "#labels = labels.astype(int)\n",
    "\n",
    "#df = pd.read_csv(dataPath, header = None)\n",
    "#features = df.iloc[1:,:-2].to_numpy()\n",
    "#labels = df.iloc[1:,-1].to_numpy() # last but one column for PD vs NC\n",
    "\n",
    "#dict_hvlv = {1:0, 2:1, 3:0, 4:0, 5:1, 6:0} #HVLV labels mapping dictionary\n",
    "#labels = labels.map(dict_hvlv).to_numpy()\n",
    "#labels[labels == 1] = 0\n",
    "#labels[labels == 2] = 1\n",
    "#labels[labels == 3] = 1\n",
    "#labels[labels == 4] = 1\n",
    "#labels[labels == 5] = 1\n",
    "#labels[labels == 6] = 1\n",
    "#labels[labels < 0]=0\n",
    "#labels[labels == 6]=0\n",
    "\n",
    "# randomise the sample sequence\n",
    "rand_order = np.arange(features.shape[0])\n",
    "np.random.shuffle(rand_order)\n",
    "features = features[rand_order,]\n",
    "labels = np.squeeze(labels[rand_order,])\n",
    "class_num = np.size(np.unique(labels))\n",
    "subject_ids = np.squeeze(subject_ids[rand_order,])\n",
    "labels_categorical = np_utils.to_categorical(labels, class_num)\n",
    "\n",
    "del matContent\n",
    "\n",
    "print('Features shape:', features.shape)\n",
    "print('Number of classes:', class_num)\n",
    "print('Unique labels:', np.unique(labels))\n",
    "print('Unique subject ids:', np.unique(subject_ids))\n",
    "\n",
    "subject_ids = (subject_ids.astype(int)).astype(str)\n",
    "subject_ids\n",
    "print('Unique subject ids:', np.unique(subject_ids))\n",
    "print(subject_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create, and compile Keras model\n",
    "def create_model(init_mode, activation, dropout_rate, optimizer, learn_rate):\n",
    "#def create_model(activation):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(filters=nb_filters[0], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, input_shape=(features.shape[1], features.shape[2], features.shape[3]), trainable=True))\n",
    "  model.add(AveragePooling2D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  model.add(Conv2D(filters=nb_filters[1], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, kernel_initializer=init_mode, trainable=True))\n",
    "  model.add(AveragePooling2D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  model.add(Conv2D(filters=nb_filters[2], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, kernel_initializer=init_mode, trainable=True))\n",
    "  model.add(AveragePooling2D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  # ####added by me#####\n",
    "  #model.add(Conv1D(filters=nb_filters[3], kernel_size=kernel_size, padding=padding, activation=activation,\n",
    "  #              kernel_initializer='he_normal'))\n",
    "  #model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  #model.add(Conv1D(filters=nb_filters[4], kernel_size=kernel_size, padding=padding, activation=activation,\n",
    "  #              kernel_initializer='he_normal'))\n",
    "  #model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  # ####added by me#####\n",
    "  model.add(Flatten())\n",
    "  model.add(BatchNormalization(epsilon=0.001))\n",
    "  model.add(Dense(dense_layer_neuron_num, kernel_initializer=init_mode, activation=activation))\n",
    "  model.add(Dropout(dropout_rate))\n",
    "  model.add(Dense(class_num))\n",
    "  model.add(Activation('softmax'))\n",
    "  #model.summary()\n",
    "  #model.load_weights('Gender_notClean_HIweights.hdf5')\n",
    "  #earlyStopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "  if optimizer == 'SGD':\n",
    "    opt = SGD(learning_rate=learn_rate / 10 ** epochs, momentum = momentum, decay = weight_decay, nesterov = True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  elif optimizer == 'Adam':\n",
    "    opt = Adam()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  elif optimizer == 'RMSprop':\n",
    "    opt = RMSprop(learning_rate=learn_rate, epsilon=1e-07)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_path = '../results/CNN_2D_our/CNN_2D_results_MultiClass_NC.mat'\n",
    "params = sio.loadmat(mat_path)\n",
    "best_params = params['best_params']\n",
    "del mat_path\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learn_rate = [0.00001, 0.0001, 0.001]\n",
    "optimizer = ['SGD', 'Adam','RMSprop']\n",
    "#momentum = [0.8,0.9]\n",
    "init_mode = ['he_normal','he_uniform']\n",
    "activation = ['relu','tanh']\n",
    "dropout_rate = [0.3,0.4,0.5]\n",
    "foldNum = 10\n",
    "\n",
    "p_grid = dict(init_mode=init_mode, dropout_rate=dropout_rate, activation=activation,\n",
    "              optimizer=optimizer, learn_rate=learn_rate)\n",
    "              #, momentum=momentum)\n",
    "\n",
    "conf_mat = np.zeros((2,2))\n",
    "#conf_mat = np.zeros((6,6))\n",
    "f = 0\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "f1_MacroNet = np.zeros([foldNum,]) \n",
    "f1_weightedNet = np.zeros([foldNum,])\n",
    "precisionNet = np.zeros([foldNum,])\n",
    "recallNet = np.zeros([foldNum,])\n",
    "accNet = np.zeros([foldNum,])\n",
    "\n",
    "#channels = features.shape[2] # number of channels 14\n",
    "# Use Stratified K Fold to make sure that the train data and test data split distribution match.\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)\n",
    "\n",
    "#features = np.reshape(features, (features.shape[0],640,14)) #reshaped to (None, 3, 14) for spectral\n",
    "\n",
    "cm_list = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    trainingFeatures = features[train,:,:]\n",
    "    testFeatures = features[test,:,:]\n",
    "    train_shape = trainingFeatures.shape\n",
    "    test_shape = testFeatures.shape\n",
    "    \n",
    "    # Standerdize \n",
    "    scaler = StandardScaler()\n",
    "    trainingFeatures = np.reshape(trainingFeatures, [train_shape[0], train_shape[1]*train_shape[2]])\n",
    "    testFeatures = np.reshape(testFeatures, [test_shape[0], test_shape[1]*test_shape[2]])\n",
    "    scaler.fit(trainingFeatures)\n",
    "    trainingFeatures = scaler.transform(trainingFeatures)\n",
    "    trainingFeatures = np.reshape(trainingFeatures, [train_shape[0],train_shape[1],train_shape[2]])\n",
    "    testFeatures = scaler.transform(testFeatures)\n",
    "    testFeatures = np.reshape(testFeatures,[test_shape[0], test_shape[1], test_shape[2]])\n",
    "    \n",
    "    cnn_model = KerasClassifier(build_fn=create_model, verbose=1)  # Call the classifier\n",
    "    \n",
    "    grid = GridSearchCV(estimator=cnn_model, param_grid=p_grid,\n",
    "                    cv=kfold.split(trainingFeatures, y_train), verbose=0)\n",
    "    \n",
    "    grid_result = grid.fit(trainingFeatures,labels[train,])\n",
    "    best_params = grid_result.best_params_\n",
    "    print('Best parameters:', best_params)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    estimator = create_model(init_mode=best_params.get('init_mode'),\n",
    "                         learn_rate=best_params.get('learn_rate'),\n",
    "                         optimizer=best_params.get('optimizer'),\n",
    "                         #momentum=best_params.get('momentum'),\n",
    "                         activation=best_params.get('activation'),\n",
    "                         dropout_rate=best_params.get('dropout_rate'))\n",
    "\n",
    "    estimator.summary()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trainingFeatures, labels_categorical[train,:], test_size=0.1,\n",
    "                                                      random_state=100, stratify=labels_categorical[train,:])\n",
    "    #earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "    dummy = estimator.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=epochs, \n",
    "                          verbose=2, validation_split=0.1)\n",
    "    \n",
    "    predicted_labelsNet = estimator.predict_classes(testFeatures, verbose=0)\n",
    "    cm = confusion_matrix(labels[test,], predicted_labelsNet, labels=[0,1])\n",
    "    #cm = confusion_matrix(labels[test,], predicted_labelsNet, labels=[1,2,3,4,5,0])\n",
    "    cm = cm/cm.sum(axis=1, keepdims=True)\n",
    "    #conf_mat = conf_mat+cm\n",
    "    cm_list.append(cm)\n",
    "\n",
    "    precisionNet[f] = precision_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    recallNet[f] = recall_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    f1_MacroNet[f] = f1_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    f1_weightedNet[f] = f1_score(labels[test,], predicted_labelsNet, average='weighted')\n",
    "    accNet[f] = accuracy_score(labels[test,], predicted_labelsNet)\n",
    "    print(experiment + '_CNN: Fold %d : f1_macroscore: %.4f' % (f + 1, f1_MacroNet[f]))\n",
    "    print(experiment + '_CNN: Fold %d : f1_weightedscore: %.4f' % (f + 1, f1_weightedNet[f]))\n",
    "    print(experiment + '_CNN: Fold %d : acc: %.4f' % (f + 1, accNet[f]))\n",
    "    f += 1\n",
    "    train_loss.append(dummy.history['loss'])\n",
    "    val_loss.append(dummy.history['val_loss'])\n",
    "    train_accuracy.append(dummy.history['accuracy'])\n",
    "    val_accuracy.append(dummy.history['val_accuracy'])\n",
    "    \n",
    "    \n",
    "#xc = range(1,epochs+1)\n",
    "tr_loss_df = pd.DataFrame(train_loss)\n",
    "tr_acc_df = pd.DataFrame(train_accuracy)\n",
    "val_loss_df = pd.DataFrame(val_loss)\n",
    "val_acc_df = pd.DataFrame(val_accuracy)\n",
    "\n",
    "train_loss_mlist = tr_loss_df.mean(axis=0).to_numpy()\n",
    "train_loss_slist = tr_loss_df.std(axis=0).to_numpy()\n",
    "train_acc_mlist = tr_acc_df.mean(axis=0).to_numpy() \n",
    "train_acc_slist = tr_acc_df.std(axis=0).to_numpy() \n",
    "val_loss_mlist = val_loss_df.mean(axis=0).to_numpy() \n",
    "val_loss_slist = val_loss_df.std(axis=0).to_numpy() \n",
    "val_acc_mlist = val_acc_df.mean(axis=0).to_numpy()\n",
    "val_acc_slist = val_acc_df.std(axis=0).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "y1 = train_loss_mlist\n",
    "e1 = train_loss_slist\n",
    "y2 = val_loss_mlist\n",
    "e2 = val_loss_slist\n",
    "x1 = range(1, len(train_loss_mlist) + 1)\n",
    "a1 = plt.plot(x1, y1, color='#089FFF', label='Training Loss')\n",
    "plt.fill_between(x1, y1 - e1, y1 + e1, alpha=0.3, facecolor='#089FFF', linewidth=4)\n",
    "a2 = plt.plot(x1, y2, color='#cc9106', label='Validation Loss')\n",
    "plt.fill_between(x1, y2 - e2, y2 + e2, alpha=0.3, facecolor='#cc9106', linewidth=4)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y3 = train_acc_mlist\n",
    "e3 = train_acc_slist\n",
    "y4 = val_acc_mlist\n",
    "e4 = val_acc_slist\n",
    "x2 = range(1, len(train_acc_mlist) + 1)\n",
    "a1 = plt.plot(x2, y3, color='#089FFF', label='Training Accuracy')\n",
    "plt.fill_between(x2, y3 - e3, y3 + e3, alpha=0.3, facecolor='#089FFF', linewidth=4)\n",
    "a2 = plt.plot(x2, y4, color='#cc9106', label='Validation Accuracy')\n",
    "plt.fill_between(x2, y4 - e4, y4 + e4, alpha=0.3, facecolor='#cc9106', linewidth=4)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(filename[:-4]+'_loss_acc_curve'+'.png')\n",
    "plt.show()\n",
    "\n",
    "#conf_mat /= conf_mat.sum(axis=1, keepdims=True) #Normalised values in the CM\n",
    "conf_mat = np.mean(cm_list, axis=0)\n",
    "#fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.heatmap(conf_mat, cmap='YlGnBu', annot=True, fmt='.4f', vmin=0, vmax=1, annot_kws={'fontsize': 12})\n",
    "ax.set_yticklabels(['NC', 'PD'], rotation=0)\n",
    "ax.set_xticklabels(['NC', 'PD'], rotation=0)\n",
    "#ax.set_yticklabels(['Sad', 'Happy', 'Fear', 'Disgust', 'Surprise', 'Anger'], rotation = 0)\n",
    "#ax.set_xticklabels(['Sad', 'Happy', 'Fear', 'Disgust', 'Surprise', 'Anger'], rotation = 0)\n",
    "\n",
    "ax.set_title(plot_title)\n",
    "ax.get_figure().savefig(filename[:-4] + '_conf_mat' + '.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mean and std of F1 MACRO is %.4f +- %.4f' % (np.mean(f1_MacroNet), np.std(f1_MacroNet)))\n",
    "print('Mean and std of F1 WEIGHTED is %.4f +- %.4f' % (np.mean(f1_weightedNet), np.std(f1_weightedNet)))\n",
    "print('Mean and std of accuracy is %.4f +- %.4f' % (np.mean(accNet), np.std(accNet)))\n",
    "\n",
    "# Save results\n",
    "sio.savemat(filename, {'precisionNet': precisionNet, 'recallNet': recallNet, 'f1_MacroNet': f1_MacroNet,\n",
    "                       'f1_weightedNet': f1_weightedNet, 'accNet': accNet, 'conf_mat': conf_mat,\n",
    "                       'conf_mat_list': cm_list,\n",
    "                       'best_params': best_params, 'experiment': experiment, 'nb_filters': nb_filters,\n",
    "                       'kernel_size': kernel_size, 'pool_size': pool_size, 'stride_size': stride_size,\n",
    "                       'padding': padding,\n",
    "                       'weight_decay': weight_decay, 'dense_layer_neuron_num': dense_layer_neuron_num, 'epochs': epochs,\n",
    "                       'train_loss': train_loss, 'train_accuracy': train_accuracy, 'val_loss': val_loss,\n",
    "                       'val_accuracy': val_accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
